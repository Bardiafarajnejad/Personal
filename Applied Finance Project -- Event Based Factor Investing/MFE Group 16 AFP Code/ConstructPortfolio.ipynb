{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bardiafarajnejad/Desktop/1st Half MFE/AFP/Final Delivery/MFE Group 16 AFP Code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "path=\"/Users/bardiafarajnejad/Desktop/1st Half MFE/AFP/Final Delivery/MFE Group 16 AFP Code\"\n",
    "os.chdir(path)\n",
    "print(os.getcwd())  #check if correct wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(flag=False): #run this function once to get the data!\n",
    "    if(flag):\n",
    "        data = pd.read_csv('afp_data_sample_1995_onward_v2.csv')\n",
    "        data\n",
    "\n",
    "        data.columns.values #view column names\n",
    "\n",
    "\n",
    "        ## First, generate daily returns for universe\n",
    "\n",
    "        ### Note:\n",
    "        ### pricepctchgd is the raw ret on adj close\n",
    "        ### so, \"ret\" OR mkt cap growth = pricepctchgd\n",
    "\n",
    "        #### Should use unadjustedpriceclose for pricepctchgd, which is what is going on above ^ (back when it was checked)\n",
    "\n",
    "        ## Next, generate 1 week, 2 week, 3 week, and 4 week returns for universe\n",
    "        ## (RECALL: MAX HOLDING PERIOD O'NEIL TEAM WANTS IS 10 DAYS, ie 1-2 weeks)\n",
    "\n",
    "        data['date'] = pd.to_datetime(data['tradedate'], format='%Y%m%d', errors='ignore')\n",
    "        data['week_count'] = (data['date']-datetime.datetime(1900,1,1)).dt.days // 7 +1 #acutally, we wont use this since we want rolling weekly returns, but it is useful to have just in case\n",
    "        data['2week_count'] = (data['date']-datetime.datetime(1900,1,1)).dt.days // 14 +1 #acutally, we wont use this since we want rolling weekly returns, but it is useful to have just in case\n",
    "        data['3week_count'] = (data['date']-datetime.datetime(1900,1,1)).dt.days // 21 +1 #acutally, we wont use this since we want rolling weekly returns, but it is useful to have just in case\n",
    "        data['4week_count'] = (data['date']-datetime.datetime(1900,1,1)).dt.days // 28 +1 #acutally, we wont use this since we want rolling weekly returns, but it is useful to have just in case\n",
    "\n",
    "        data = data.sort_values(['osid','tradedate']).reset_index(drop=True).copy()\n",
    "\n",
    "        data\n",
    "\n",
    "\n",
    "        #Next, compute rolling/moving weekly returns:\n",
    "        data['pricepctchgd'] = data['pricepctchgd']/100 #convert percentage returns to decimal returns\n",
    "\n",
    "\n",
    "        #rolling weekly returns (1 week = 5 trading days backward)\n",
    "        #NOTE: we must avoid 5 days turning into many months because of missing data in between day 1 and day 5\n",
    "        #for example, IF (day 1 = November 1st, 1999) and (day 2 = December 1st, 1999), \n",
    "        #THEN we should not bridge the gap and compute rolling return...\n",
    "        #so we will restrict the rolling return only to the times when week.diff(1) = 1 OR week.diff(1) = 0\n",
    "        #and, of course, we will group by OSID so to not compute rolling ret over 2 different stocks...\n",
    "\n",
    "        data['cumret']=0\n",
    "        data['cumalpha']=0\n",
    "        for i in range(0,20): #0-19: 20 trading days ~ 4 weeks\n",
    "            print(i,end=\",\") #to keep track of slow loop\n",
    "            #FIRST, CHECK IF LAG WEEK == WEEK \n",
    "            data['r_shifted'] = data[['osid','pricepctchgd']].groupby(['osid'])['pricepctchgd'].shift(i) #group by OSID\n",
    "            data['alpha_shifted'] = data[['osid','alpha']].groupby(['osid'])['alpha'].shift(i) #group by OSID\n",
    "\n",
    "            data['cumret'] = (1+data['cumret'])*(1+data['r_shifted']) -1\n",
    "            data['cumalpha'] = (1+data['cumalpha'])*(1+data['alpha_shifted']) -1\n",
    "            #Note: taking avg is like weekly regression except for Beta differences throughout the week\n",
    "\n",
    "            if i in [1-1,5-1,10-1,15-1,20-1]:\n",
    "                data['ret'+str(i+1)+'d'] = data['cumret']\n",
    "                data['alpha'+str(i+1)+'d'] = data['cumalpha']\n",
    "\n",
    "        data.drop(['cumret','r_shifted','cumalpha','alpha_shifted'], axis=1, inplace=True)\n",
    "        data\n",
    "\n",
    "\n",
    "        #### NOW, restrict the rolling return only to the times when week.diff(1) = 0 OR week.diff(1) = 1\n",
    "        #sort the data first\n",
    "        data = data.sort_values(['osid','tradedate']).reset_index(drop=True).copy()\n",
    "        #now that the data is sorted, we can just check at which times week.diff(1) is NOT <=1\n",
    "        xx = list((data['date'].diff(1)).dt.days >20)\n",
    "        yy = list((data['osid'].diff(1)) ==0)\n",
    "        zz = [(a and b) for a, b in zip(xx, yy)]\n",
    "        zz\n",
    "\n",
    "        print('\\nThere are ', sum(zz), ' problem rows') #these 110 dates are problematic\n",
    "\n",
    "        problem_rows = list(np.where(zz)[0]) #gets the row index of the rows for which zz is True\n",
    "        #problem_rows\n",
    "\n",
    "        problem_rows2 = [i-1 for i in problem_rows]\n",
    "\n",
    "        problem_rows3 = np.sort(problem_rows + problem_rows2)\n",
    "        problem_rows3 #this is the indices we want to check to confirm that there is a large gap in the date (for example, instead of being the next day, it is instead the next year)\n",
    "\n",
    "\n",
    "        ###########################################################################################\n",
    "        ###########################################################################################\n",
    "        ###########################################################################################\n",
    "        #this one will take a long time to run\n",
    "\n",
    "\n",
    "        data['index'] = list(data.index) #create a col called index\n",
    "        data[data.index.isin(problem_rows3)] \n",
    "        #Now, investigate why df['tradedate'].diff(1) IS NOT < 20 days and make sure all 120 exceptions are valid\n",
    "        data.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        #after investigating, looks like all exceptions are valid and that ret5d,ret10d, ret15d, and ret20d should be NaN'd out\n",
    "        for i in problem_rows:\n",
    "            \n",
    "            for j in range(1): #0\n",
    "                if( (data['osid'].diff(1))[i+j] ==0 ): #make sure we are still in one unique OSID\n",
    "                    data.loc[i+j,'ret1d'] = np.nan\n",
    "                    data.loc[i+j,'alpha1d'] = np.nan\n",
    "        #j iterates 0 days forward, \n",
    "        #i+j days forward could potentially have a corrupted return that used the return on spot i-1\n",
    "\n",
    "\n",
    "        \n",
    "            for j in range(4): #0,1,2,3\n",
    "                if( (data['osid'].diff(1))[i+j] ==0 ): #make sure we are still in one unique OSID\n",
    "                    data.loc[i+j,'ret5d'] = np.nan\n",
    "                    data.loc[i+j,'alpha5d'] = np.nan\n",
    "        #j iterates 4 days forward, \n",
    "        #i+j days forward could potentially have a corrupted return that used the return on spot i-1\n",
    "\n",
    "\n",
    "            for j in range(9): #0,1,2,3,...,8\n",
    "                if( (data['osid'].diff(1))[i+j] ==0 ): #make sure we are still in one unique OSID\n",
    "                    data.loc[i+j,'ret10d'] = np.nan\n",
    "                    data.loc[i+j,'alpha10d'] = np.nan\n",
    "        #j iterates 9 days forward, \n",
    "        #i+j days forward could potentially have a corrupted return that used the return on spot i-1\n",
    "\n",
    "            for j in range(14): #0,1,2,3,...,13\n",
    "                if( (data['osid'].diff(1))[i+j] ==0 ): #make sure we are still in one unique OSID\n",
    "                    data.loc[i+j,'ret15d'] = np.nan\n",
    "                    data.loc[i+j,'alpha15d'] = np.nan\n",
    "        #j iterates 14 days forward, \n",
    "        #i+j days forward could potentially have a corrupted return that used the return on spot i-1\n",
    "\n",
    "\n",
    "            for j in range(19): #0,1,2,3,...,19\n",
    "                if( (data['osid'].diff(1))[i+j] ==0 ): #make sure we are still in one unique OSID\n",
    "                    data.loc[i+j,'ret20d'] = np.nan\n",
    "                    data.loc[i+j,'alpha20d'] = np.nan\n",
    "        #j iterates 19 days forward, \n",
    "        #i+j days forward could potentially have a corrupted return that used the return on spot i-1\n",
    "\n",
    "\n",
    "\n",
    "        #data[data.index.isin(problem_rows3)] #this is to visually check \n",
    "\n",
    "        ###########################################################################################\n",
    "        ###########################################################################################\n",
    "        ###########################################################################################\n",
    "\n",
    "        #for example, we have the following for ret1d, ret5d, ret10d, ret15d, and ret20d\n",
    "        example_row = problem_rows3[-1].copy()\n",
    "        data[example_row-1:example_row+22]  #run this line to see what the result looks like if interested\n",
    "        \n",
    "        \n",
    "        ##############################################################################################################\n",
    "        ##################### (BELOW) GET lagged returns and alphas for 5,10,15,&20 day horizons #####################\n",
    "        ##############################################################################################################\n",
    "        #Idea:\n",
    "        #Lets lag ret5d by 5 days so it represents forward looking returns instead of backward looking returns\n",
    "        #that is, right now 'ret5d' (at time t) is the trailing 5 day cumulitive return (from t-4,t-3,t-2,t-1,t)\n",
    "        #we want to create another vairable called 'ret5d_lag'\n",
    "        #that is, 'ret5d_lag' (at time t) will represent the 5 day cumulitive return (on t+1,t+2,t+3,t+4,t+5)\n",
    "\n",
    "        for i in [1,5,10,15,20]:\n",
    "            data['ret'+str(i)+'d_lag'] = data[['osid','ret'+str(i)+'d']].groupby(['osid'])['ret'+str(i)+'d'].shift(-i) #group by OSID, shift 5d by 5, 10d by 10, etc...\n",
    "            data['alpha'+str(i)+'d_lag'] = data[['osid','alpha'+str(i)+'d']].groupby(['osid'])['alpha'+str(i)+'d'].shift(-i) #group by OSID\n",
    "\n",
    "        data\n",
    "\n",
    "        #print(data['ret5d_lag'][0:10])\n",
    "        #print(data['ret5d'][0:15])  #use this line and the one above to see the results\n",
    "\n",
    "        ##############################################################################################################\n",
    "        ##################### (ABOVE) GOT lagged returns and alphas for 5,10,15,&20 day horizons #####################\n",
    "        ##############################################################################################################\n",
    "\n",
    "        #save data to use later without running all the code above again every time\n",
    "        data.to_pickle('data_with_rets.pkl')\n",
    "        ### Now we have ret5d, ret10d, ret15d, and ret20d for valid indices only\n",
    "        ### Now we have alpha5d, alpha10d, alpha15d, and alpha20d for valid indices only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_on_valid_lags_only(data):\n",
    "    \n",
    "    #VALID LAGS ONLY!\n",
    "    #same company, but longer than 20 day difference between observations --> don't trade even if 'trade_tomorrow'==1\n",
    "\n",
    "    xx = list(abs((data[['date','osid']].groupby(['osid'])['date'].diff(-1)).dt.days) > 20) # abs(today - tomorrow)>20\n",
    "    yy = list(data['buy_tomorrow']>0) #signal is saying buy, can be =1, or can be =2 or =3 if it is moved on 2 or 3 indices at once\n",
    "    zz = list(data['sell_tomorrow']>0) #signal is saying sell\n",
    "\n",
    "    zz_buy = [(a and b) for a, b in zip(xx, yy)] #this is where you should not trade!\n",
    "    zz_sell = [(a and b) for a, b in zip(xx, zz)] #this is where you should not trade!\n",
    "\n",
    "    print('\\nThere are ', sum(zz_buy), ' problems for buying')\n",
    "    print('\\nThere are ', sum(zz_sell), ' problems for selling')\n",
    "\n",
    "\n",
    "    problem_rowsa = list(np.where(zz_buy)[0]) #gets the row index of the rows for which zz is True\n",
    "    problem_rowsb = list(np.where(zz_sell)[0]) #gets the row index of the rows for which zz is True\n",
    "    problem_rows = problem_rowsa + problem_rowsb\n",
    "\n",
    "    problem_rows2 = [i+1 for i in problem_rows]\n",
    "\n",
    "    problem_rows3 = np.sort(problem_rows + problem_rows2)\n",
    "    problem_rows3 #this is the indices we want to check to confirm that there is a large gap in the date (for example, instead of being the next day, it is instead the next year)\n",
    "\n",
    "\n",
    "    data['index'] = list(data.index) #create a col called index\n",
    "    data[data.index.isin(problem_rows3)][['tradedate','symbol', 'coname','volume','avgvol50d','buy_tomorrow', 'sell_tomorrow']]  #THESE ROWS ARE PROBLEMATIC!\n",
    "\n",
    "    #Now, investigate why df['tradedate'].diff(1) IS NOT < 20 days and make sure all 120 exceptions are valid\n",
    "    #data.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "# WE SHOULDN'T TRADE ON ANY OF THE EVENTS ABOVE!!! \n",
    "#Because the dates jump from one year to another\n",
    "\n",
    "#after investigating, looks like all exceptions are valid \n",
    "#and that we shouldn't trade on the events from above\n",
    "\n",
    "    data.loc[data.index.isin(problem_rowsa),'buy_tomorrow'] = 0 #erase signal and dont trade on it\n",
    "    data.loc[data.index.isin(problem_rowsb),'sell_tomorrow'] = 0 #erase signal and dont trade on it\n",
    "\n",
    "    data[data.index.isin(problem_rows3)][['tradedate','symbol', 'coname','sp100f','sp500f','nasdaq100f','buy_tomorrow', 'sell_tomorrow']]  #THESE ROWS ARE PROBLEMATIC!\n",
    "    #data.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "    print('Total number of Buys: ',np.count_nonzero(data['buy_tomorrow']))\n",
    "    print('Total number of Sells: ',np.count_nonzero(data['sell_tomorrow']))\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_signal_forward(data):\n",
    "    data['ones'] = 1\n",
    "    data_smaller = data.copy()\n",
    "    data_smaller = data_smaller.loc[data_smaller['ret1d_lag'].notna(),:].reset_index(drop=True).copy()  #DROP MISSING RETURNS\n",
    "    data_smaller = data_smaller.loc[data_smaller['alpha1d_lag'].notna(),:].reset_index(drop=True).copy()  #DROP MISSING ALPHAS\n",
    "\n",
    "    data_smaller['index'] = list(data_smaller.index) #create a col called index\n",
    "\n",
    "    ################################################################################################################\n",
    "    ################################################################################################################\n",
    "    #Goal: make this part much faster!\n",
    "    #VALID LAGS ONLY! THIS WILL TAKE A LONG TIME TO RUN\n",
    "    #same company, AND smaller than 20 day difference in-between observations --> set 'trade_tomorrow'=1 in the next 4,9,14,19 days forward\n",
    "\n",
    "    xx = list(abs((data_smaller[['date','osid']].groupby(['osid'])['date'].diff(-1)).dt.days) < 20) # abs(today - tomorrow)<20\n",
    "    yy = list(data_smaller['buy_tomorrow']>0) #signal is saying buy, can be =1, or can be =2 or =3 if it is moved on 2 or 3 indices at once\n",
    "    zz = list(data_smaller['sell_tomorrow']>0) #signal is saying sell\n",
    "\n",
    "    zz_buy = [(a and b) for a, b in zip(xx, yy)] #this is where you should bring buy_signal forward!\n",
    "    zz_sell = [(a and b) for a, b in zip(xx, zz)] #this is where you should bring sell_signal forward!\n",
    "\n",
    "    rowsa = list(np.where(zz_buy)[0]) #gets the row index of the rows for which we should bring buy_signal forward\n",
    "    rowsb = list(np.where(zz_sell)[0]) #gets the row index of the rows for which we should bring sell_signal forward\n",
    "\n",
    "    ################################################################################################################\n",
    "\n",
    "    #now, ALSO create data_smaller5, data_smaller10, data_smaller15, data_smaller20: holding periods of 5,10,15,20 days\n",
    "    data_smaller5 = data_smaller.copy()\n",
    "    for i in range(1,5): #lag signal i=1,2,3,4 days forward\n",
    "        data_smaller5['buy_tomorrow'] += np.where(data_smaller5[['osid','index']].groupby(['osid'])['index'].shift(i).isin(rowsa), 1, 0)\n",
    "        data_smaller5['sell_tomorrow'] += np.where(data_smaller5[['osid','index']].groupby(['osid'])['index'].shift(i).isin(rowsb), 1, 0)\n",
    "        #if index (i spots above) is in rowsa/OR/rowsb, then set 'buy_tomorrow'/'sell_tomorrow'=1\n",
    "\n",
    "    data_smaller10 = data_smaller5.copy()\n",
    "    for i in range(5,10): #lag signal i=5,...,9 days forward\n",
    "        data_smaller10['buy_tomorrow'] += np.where(data_smaller10[['osid','index']].groupby(['osid'])['index'].shift(i).isin(rowsa), 1, 0)\n",
    "        data_smaller10['sell_tomorrow'] += np.where(data_smaller10[['osid','index']].groupby(['osid'])['index'].shift(i).isin(rowsb), 1, 0)\n",
    "        #if index (i spots above) is in rowsa/OR/rowsb, then set 'buy_tomorrow'/'sell_tomorrow'=1\n",
    "\n",
    "    data_smaller15 = data_smaller10.copy()\n",
    "    for i in range(10,15): #lag signal i=10,...,14 days forward\n",
    "        data_smaller15['buy_tomorrow'] += np.where(data_smaller15[['osid','index']].groupby(['osid'])['index'].shift(i).isin(rowsa), 1, 0)\n",
    "        data_smaller15['sell_tomorrow'] += np.where(data_smaller15[['osid','index']].groupby(['osid'])['index'].shift(i).isin(rowsb), 1, 0)\n",
    "        #if index (i spots above) is in rowsa/OR/rowsb, then set 'buy_tomorrow'/'sell_tomorrow'=1\n",
    "\n",
    "    data_smaller20 = data_smaller15.copy()\n",
    "    for i in range(15,20): #lag signal i=15,...,19 days forward\n",
    "        data_smaller20['buy_tomorrow'] += np.where(data_smaller20[['osid','index']].groupby(['osid'])['index'].shift(i).isin(rowsa), 1, 0)\n",
    "        data_smaller20['sell_tomorrow'] += np.where(data_smaller20[['osid','index']].groupby(['osid'])['index'].shift(i).isin(rowsb), 1, 0)\n",
    "        #if index (i spots above) is in rowsa/OR/rowsb, then set 'buy_tomorrow'/'sell_tomorrow'=1\n",
    "\n",
    "    return(data_smaller,data_smaller5,data_smaller10,data_smaller15,data_smaller20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_summ(ew_alphas,ew_returns, horizon_days):\n",
    "    #horizon_days=1,5,10,15,20\n",
    "    answers = np.zeros(8)\n",
    "    answers[0] = (252/horizon_days)*100*ew_alphas.mean()\n",
    "    answers[1] = np.sqrt((252/horizon_days)) * (100) * np.sqrt( np.sum(ew_alphas**2)/(len(ew_alphas)-1) )\n",
    "    answers[2] = answers[0]/answers[1]\n",
    "    answers[3] = ew_alphas.skew()\n",
    "\n",
    "    answers[4] = (252/horizon_days)*100*ew_returns.mean()\n",
    "    answers[5] = np.sqrt((252/horizon_days)) * (100) * np.sqrt( np.sum((ew_returns-np.mean(ew_returns))**2)/(len(ew_returns)-1) )\n",
    "    answers[6] = answers[4]/answers[5]\n",
    "    answers[7] = ew_returns.skew()\n",
    "                 \n",
    "    return(answers)\n",
    "\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "\n",
    "def get_portfolio_returns(data_smaller):\n",
    "    data_buy = data_smaller[data_smaller['buy_tomorrow']>0]\n",
    "\n",
    "    #without weights\n",
    "    ew_returns_buy2 = data_buy.groupby(['date'])['ret1d_lag'].mean().to_frame().reset_index().rename(columns={'ret1d_lag':'ew_return'})\n",
    "    ew_returns_buy2['ew_return'] = round(ew_returns_buy2['ew_return'],4) #round to 4 decimal digits\n",
    "    ew_returns_buy2\n",
    "\n",
    "    ew_alphas_buy2 = data_buy.groupby(['date'])['alpha1d_lag'].mean().to_frame().reset_index().rename(columns={'alpha1d_lag':'ew_alpha'})\n",
    "    ew_alphas_buy2['ew_alpha'] = round(ew_alphas_buy2['ew_alpha'],4) #round to 4 decimal digits\n",
    "    ew_alphas_buy2\n",
    "\n",
    "    ##with weights\n",
    "    w = data_buy.groupby(['date'])['ones'].sum().to_frame().reset_index().rename(columns={'ones':'N'})\n",
    "    w['weight'] = 1/w['N']\n",
    "    data_buy = data_buy.merge(w, how='left', on='date').copy()\n",
    "    data_buy['ew_return_contr'] = data_buy['ret1d_lag'] * data_buy['weight'] #return contribtuion !\n",
    "    ew_returns_buy = data_buy.groupby(['date'])['ew_return_contr'].sum().to_frame().reset_index().rename(columns={'ew_return_contr':'ew_return'})\n",
    "    ew_returns_buy['ew_return'] = round(ew_returns_buy['ew_return'],4) #round to 4 decimal digits\n",
    "    #print(ew_returns_buy)\n",
    "\n",
    "    data_buy['ew_alpha_contr'] = data_buy['alpha1d_lag'] * data_buy['weight'] #return contribtuion !\n",
    "    ew_alphas_buy = data_buy.groupby(['date'])['ew_alpha_contr'].sum().to_frame().reset_index().rename(columns={'ew_alpha_contr':'ew_alpha'})\n",
    "    ew_alphas_buy['ew_alpha'] = round(ew_alphas_buy['ew_alpha'],4) #round to 4 decimal digits\n",
    "    #print(ew_returns_buy)\n",
    "\n",
    "    print('\\nThere are ',sum(ew_returns_buy['ew_return'] != ew_returns_buy2['ew_return']),' returns that dont match using .mean() vs. using equal weights')\n",
    "    print('\\nThere are ',sum(ew_alphas_buy['ew_alpha'] != ew_alphas_buy2['ew_alpha']),' alphas that dont match using .mean() vs. using equal weights')\n",
    "\n",
    "    #for i in range(len(ew_returns_buy['ew_return'])):\n",
    "    #    if(ew_returns_buy['ew_return'][i] != ew_returns_buy2['ew_return'][i]):\n",
    "    #        print(ew_returns_buy['ew_return'][i], ' != ', ew_returns_buy2['ew_return'][i])\n",
    "    #these numbers actually match and are no problem !\n",
    "\n",
    "    ew_returns_buy = ew_returns_buy.sort_values(by='date')\n",
    "\n",
    "    ew_alphas_buy = ew_alphas_buy.sort_values(by='date')\n",
    "\n",
    "    ew_returns_buy['ew_alpha'] = ew_alphas_buy['ew_alpha']\n",
    "    ew_returns_buy\n",
    "\n",
    "    ####################################################################################################################################\n",
    "    ####################################################################################################################################\n",
    "    ####################################################################################################################################\n",
    "\n",
    "    data_sell = data_smaller[data_smaller['sell_tomorrow']>0]\n",
    "\n",
    "    #without weights\n",
    "    ew_returns_sell2 = data_sell.groupby(['date'])['ret1d_lag'].mean().to_frame().reset_index().rename(columns={'ret1d_lag':'ew_return'})\n",
    "    ew_returns_sell2['ew_return'] = round(ew_returns_sell2['ew_return'],4) #round to 4 decimal digits\n",
    "    ew_returns_sell2\n",
    "\n",
    "    ew_alphas_sell2 = data_sell.groupby(['date'])['alpha1d_lag'].mean().to_frame().reset_index().rename(columns={'alpha1d_lag':'ew_alpha'})\n",
    "    ew_alphas_sell2['ew_alpha'] = round(ew_alphas_sell2['ew_alpha'],4) #round to 4 decimal digits\n",
    "    ew_alphas_sell2\n",
    "\n",
    "\n",
    "    ##with weights\n",
    "    w = data_sell.groupby(['date'])['ones'].sum().to_frame().reset_index().rename(columns={'ones':'N'})\n",
    "    w['weight'] = 1/w['N']\n",
    "    data_sell = data_sell.merge(w, how='left', on='date').copy()\n",
    "    data_sell['ew_return_contr'] = data_sell['ret1d_lag'] * data_sell['weight'] #return contribtuion !\n",
    "    ew_returns_sell = data_sell.groupby(['date'])['ew_return_contr'].sum().to_frame().reset_index().rename(columns={'ew_return_contr':'ew_return'})\n",
    "    ew_returns_sell['ew_return'] = round(ew_returns_sell['ew_return'],4) #round to 4 decimal digits\n",
    "    #print(ew_returns_sell)\n",
    "\n",
    "    data_sell['ew_alpha_contr'] = data_sell['alpha1d_lag'] * data_sell['weight'] #return contribtuion !\n",
    "    ew_alphas_sell = data_sell.groupby(['date'])['ew_alpha_contr'].sum().to_frame().reset_index().rename(columns={'ew_alpha_contr':'ew_alpha'})\n",
    "    ew_alphas_sell['ew_alpha'] = round(ew_alphas_sell['ew_alpha'],4) #round to 4 decimal digits\n",
    "    #print(ew_returns_buy)\n",
    "\n",
    "\n",
    "    print('\\nThere are ',sum(ew_returns_sell['ew_return'] != ew_returns_sell2['ew_return']),' returns that dont match using .mean() vs. using equal weights')\n",
    "    print('\\nThere are ',sum(ew_alphas_sell['ew_alpha'] != ew_alphas_sell2['ew_alpha']),' alphas that dont match using .mean() vs. using equal weights')\n",
    "    #for i in range(len(ew_returns_sell['ew_return'])):\n",
    "    #    if(ew_returns_sell['ew_return'][i] != ew_returns_sell2['ew_return'][i]):\n",
    "    #        print(ew_returns_sell['ew_return'][i], ' != ', ew_returns_sell2['ew_return'][i])\n",
    "    #these numbers actually match and are no problem !\n",
    "\n",
    "    ew_returns_sell = ew_returns_sell.sort_values(by='date')\n",
    "    ew_returns_sell\n",
    "\n",
    "    ew_alphas_sell = ew_alphas_sell.sort_values(by='date')\n",
    "    ew_alphas_sell\n",
    "\n",
    "    ew_returns_sell['ew_alpha'] = ew_alphas_sell['ew_alpha']\n",
    "    ew_returns_sell\n",
    "\n",
    "    ####################################################################################################################################\n",
    "    ####################################################################################################################################\n",
    "    ####################################################################################################################################\n",
    "\n",
    "    #make sure weights sum to 1 on each date that we want to trade!\n",
    "    x = sum(data_buy.groupby(['date'])['weight'].sum() < 1.0000001) #do this because weights may be 0.99999999999999999 and not =1.0\n",
    "    y = sum(data_buy.groupby(['date'])['weight'].sum() > 0.9999999) \n",
    "    total = len(data_buy.groupby(['date'])['weight'].sum())\n",
    "    if(x==total & y==total):\n",
    "        print('\\nAll weights sum to one for Buys')\n",
    "    else:\n",
    "        print('\\n!!!Weights dont sum to one for Buys!!!')\n",
    "\n",
    "\n",
    "    x = sum(data_sell.groupby(['date'])['weight'].sum() < 1.0000001) #do this because weights may be 0.99999999999999999 and not =1.0\n",
    "    y = sum(data_sell.groupby(['date'])['weight'].sum() > 0.9999999) \n",
    "    total = len(data_sell.groupby(['date'])['weight'].sum())\n",
    "    if(x==total & y==total):\n",
    "        print('\\nAll weights sum to one for Sells')\n",
    "    else:\n",
    "        print('\\n!!!Weights dont sum to one for Sells!!!')\n",
    "\n",
    "    ####################################################################################################################################\n",
    "    ####################################################################################################################################\n",
    "    ####################################################################################################################################\n",
    "\n",
    "    ew_returns_buy['cum_ret'] = (ew_returns_buy['ew_return']+1).cumprod()\n",
    "    ew_returns_buy['cum_alpha'] = (ew_returns_buy['ew_alpha']+1).cumprod()\n",
    "\n",
    "    ew_returns_sell['cum_ret'] = (ew_returns_sell['ew_return']+1).cumprod()\n",
    "    ew_returns_sell['cum_alpha'] = (ew_returns_sell['ew_alpha']+1).cumprod()\n",
    "\n",
    "    ####################################################################################################################################\n",
    "    ####################################################################################################################################\n",
    "    ####################################################################################################################################\n",
    "\n",
    "    summ_stats_buy = mini_summ(ew_returns_buy['ew_alpha'], ew_returns_buy['ew_return'], horizon_days=1)\n",
    "    summ_stats_sell = mini_summ(ew_returns_sell['ew_alpha'], ew_returns_sell['ew_return'], horizon_days=1)\n",
    "\n",
    "    return(ew_returns_buy, ew_returns_sell, data_buy, data_sell, summ_stats_buy, summ_stats_sell)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turnover(data_buy): #can also input 'data_sell' to get 'turnover_sell'!!\n",
    "    turnover_buy = data_buy[['date','osid','symbol','ret1d_lag','alpha1d_lag','buy_tomorrow','N','weight',\\\n",
    "                         'ew_return_contr','ew_alpha_contr']].sort_values(by=['osid','date']).reset_index(drop=True)\n",
    "    turnover_buy['weight_end'] = turnover_buy['weight'] * (1+turnover_buy['ret1d_lag'])\n",
    "\n",
    "    turnover_buy = turnover_buy.merge(turnover_buy[['date','weight_end']]\\\n",
    "                             .groupby(['date']).sum().reset_index().rename(columns={\"weight_end\":\"weight_end_sum\"}),\\\n",
    "                             on=['date'],how='left')\n",
    "    #group by date and sum up all the 'weight_end's to get total weight of Portfolio (now no longer =1)\n",
    "\n",
    "    turnover_buy.loc[turnover_buy['weight_end_sum'] <=0,'weight_end_sum'] = np.nan\n",
    "    turnover_buy['weight_in_portfolio_end'] = turnover_buy['weight_end'] / turnover_buy['weight_end_sum']\n",
    "\n",
    "    turnover_buy['lag_weight_in_portfolio_end'] = turnover_buy[['osid','weight_in_portfolio_end']].groupby('osid').shift(1)['weight_in_portfolio_end']\n",
    "    ##############################################################################################################\n",
    "    ##############################################################################################################\n",
    "    ##############################################################################################################\n",
    "    ### Note: IF a stock is traded today, and then not traded again for more than 15 days, \n",
    "    #THEN we should take it out of the portfolio before putting it back in \n",
    "    #(ie weight should go from + number to 0, then from 0 to a + number again)\n",
    "\n",
    "    ### That is, if a stock is traded discretely once today and never again until 1 year later, \n",
    "    #then turnover for that stock should be 100% when it is traded today\n",
    "    ##############################################################################################################\n",
    "    ##############################################################################################################\n",
    "    ##############################################################################################################\n",
    "\n",
    "    #BEFORE WE COMPUTE CHANGE IN WEIGHTS: find the problem rows when change in date > 5:\n",
    "    xx = list((turnover_buy[['date','osid']].groupby(['osid'])['date'].diff(1)).dt.days >5) #(today - yesterday) > 5\n",
    "    #IF the difference between trade dates for one stock is > 5 days\n",
    "    #THEN, we should change 'lag_weight_end' to 0 !\n",
    "\n",
    "    print('\\nThere are ', sum(xx), ' problems for turnover using lag_weight_in_portfolio_end')\n",
    "\n",
    "    problem_rows = list(np.where(xx)[0]) #gets the row index of the rows for which xx is True\n",
    "\n",
    "    problem_rows2 = [i-1 for i in problem_rows]\n",
    "\n",
    "    problem_rows3 = np.sort(problem_rows + problem_rows2)\n",
    "    problem_rows3 #this is the indices we want to check to confirm that there is a large gap in the date (for example, instead of being the next day, it is instead the next year)\n",
    "\n",
    "\n",
    "    turnover_buy['index'] = list(turnover_buy.index) #create a col called index\n",
    "    turnover_buy[turnover_buy.index.isin(problem_rows3)]\n",
    "    #THESE ROWS ARE PROBLEMATIC AND WE SHOULD NOT use their 'lag_weight_in_portfolio_end' to get T-cost!\n",
    "\n",
    "    ##############################################################################################################\n",
    "    ##############################################################################################################\n",
    "    ##############################################################################################################\n",
    "\n",
    "    #after investigating, looks like all exceptions are valid and that we shouldn't use 'lag_weight_in_portfolio_end'!\n",
    "    #DONT DO THE FOLLOWING 3 LINES BELOW\n",
    "    #for i in problem_rows:\n",
    "    #    if( (turnover_buy['osid'].diff(1))[i] ==0 ): #make sure we are still in one unique OSID\n",
    "    #        turnover_buy.loc[i,'lag_weight_in_portfolio_end'] = 0\n",
    "\n",
    "    turnover_buy.loc[turnover_buy.index.isin(problem_rows),'lag_weight_in_portfolio_end'] = 0\n",
    "    turnover_buy[turnover_buy.index.isin(problem_rows3)]\n",
    "    #THESE ROWS ARE PROBLEMATIC AND WE SHOULD NOT use their 'lag_weight_in_portfolio_end' to get T-cost!\n",
    "    #turnover_buy.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "    ##############################################################################################################\n",
    "    ##############################################################################################################\n",
    "    ##############################################################################################################\n",
    "\n",
    "    ## Now we can compute the change in weights using 'lag_weight_in_portfolio_end'\n",
    "    #computing change in portfolio weights\n",
    "    turnover_buy['delta_share'] = turnover_buy['weight'] - turnover_buy['lag_weight_in_portfolio_end'].fillna(0)\n",
    "    turnover_buy['delta_share'] = turnover_buy['delta_share'].abs()\n",
    "\n",
    "    turnover_buy = turnover_buy.sort_values(by=['date']).reset_index(drop=True)\n",
    "    turnover_buy2 = turnover_buy[['date','delta_share']].groupby(['date']).sum().reset_index().rename(columns={'delta_share':'turnover'})\n",
    "    turnover_buy2['turnover'] = round(turnover_buy2['turnover'],4)\n",
    "    turnover_buy2\n",
    "\n",
    "    print('Total number of days when turnover is not equal to 100%: ',sum(list(turnover_buy2['turnover']!=1)))\n",
    "    #listt = []\n",
    "    #for i in range(len(turnover_buy2)):\n",
    "    #    if(turnover_buy2['turnover'][i]!=1):\n",
    "    #        #print('Row ',i,' has ',turnover_buy2['turnover'][i],'% turnover')\n",
    "    #        listt.append(i)\n",
    "\n",
    "    ##### IF output is: \"Row  122  has  0.0 % turnover\" --> \n",
    "    ##### check row 122 via the following 2 lines below to make sure we are trading the same stock \n",
    "    ##### on 2 consecutive days and that turnover should truly be =0\n",
    "\n",
    "    #turnover_buy[turnover_buy['date']==turnover_buy2['date'][122-1]]\n",
    "    #turnover_buy[turnover_buy['date']==turnover_buy2['date'][122]]\n",
    "\n",
    "    ##### ^^^ This confirms that we dont have any turnover on that day ^^^\n",
    "    ##### since we ONLY (N=1 on both days) trade the SAME (symbol is the same on both days) stock on both days\n",
    "    \n",
    "    return(turnover_buy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: Now we have the following 2 ew_returns series, grouped by date:\n",
    "#### 1) ew_returns_buy #(includes both ret and alpha, cum_ret and cum_alpha, AND TURNOVER NOW!)\n",
    "#### 2) ew_returns_sell #(includes both ret and alpha, cum_ret and cum_alpha, AND TURNOVER NOW!)\n",
    "\n",
    "##### NOTE: The weights (and the return contributions) are in the following dataframes:\n",
    "#### 1) data_buy\n",
    "#### 2) data_sell\n",
    "\n",
    "##### NOTE: The summary statistics are in the following lists:\n",
    "#### 1) summ_stats_buy\n",
    "#### 2) summ_stats_sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_cost_and_capital(ew_returns_buy): #can also input 'ew_returns_sell' to get 'T_cost_sell'\n",
    "    capital = [1]\n",
    "    T_cost_buy = 0\n",
    "    alpha_capital = [1]\n",
    "\n",
    "    for i in range(len(ew_returns_buy['ew_return'])):\n",
    "        starting_capital_afterTcost = capital[i] - capital[i]*0.0005*ew_returns_buy['turnover'][i] \n",
    "        #^^5 bp when we buy * turnover when we buy^^^^^^^\n",
    "        returns = 1+ew_returns_buy['ew_return'][i]\n",
    "        ending_capital = (starting_capital_afterTcost*returns)\n",
    "\n",
    "        if(i != len(ew_returns_buy['ew_return']) - 1):\n",
    "            ending_capital_afterTcost = ending_capital - ending_capital*0.0005*ew_returns_buy['turnover'][i+1] \n",
    "            #5 bp when we sell * turnover when we sell ^^^^^^^^^^^^^^^^^^^^^^^\n",
    "        else: #assumption: 100% turnover on the last day of sample when we sell and close the position!\n",
    "            ending_capital_afterTcost = ending_capital - ending_capital*0.0005*1\n",
    "            #5 bp when we sell*(turnover=100%) when we sell (on the last day of sample)^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "        capital.append(ending_capital_afterTcost)\n",
    "        T_cost_buy += (capital[i]-starting_capital_afterTcost) + (ending_capital-ending_capital_afterTcost)\n",
    "        #take off 5bp when we buy^^^^^^^^^^^^^^^^^^^^^^^^, then take off 5bp when we sell^^^^^^^^^^^^^^^\n",
    "\n",
    "    for i in range(len(ew_returns_buy['ew_alpha'])):\n",
    "        starting_capital_afterTcost = alpha_capital[i] - alpha_capital[i]*0.0005*ew_returns_buy['turnover'][i] \n",
    "        #^^5 bp when we buy * turnover when we buy^^^^^^^\n",
    "        returns = 1+ew_returns_buy['ew_alpha'][i]\n",
    "        ending_capital = (starting_capital_afterTcost*returns)\n",
    "\n",
    "        if(i != len(ew_returns_buy['ew_alpha']) - 1):\n",
    "            ending_capital_afterTcost = ending_capital - ending_capital*0.0005*ew_returns_buy['turnover'][i+1] \n",
    "            #5 bp when we sell * turnover when we sell ^^^^^^^^^^^^^^^^^^^^^^^\n",
    "        else: #assumption: 100% turnover on the last day of sample when we sell and close the position!\n",
    "            ending_capital_afterTcost = ending_capital - ending_capital*0.0005*1\n",
    "            #5 bp when we sell*(turnover=100%) when we sell (on the last day of sample)^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "        alpha_capital.append(ending_capital_afterTcost)\n",
    "\n",
    "\n",
    "    del capital[0] #remove the initial capital 1 so that the size of capital fits in df 'ew_returns_buy'\n",
    "    del alpha_capital[0] #remove the initial capital 1 so that the size of capital fits in df 'ew_returns_buy'\n",
    "    ew_returns_buy['capital'] = capital\n",
    "    ew_returns_buy['alpha_capital'] = alpha_capital\n",
    "\n",
    "    return(ew_returns_buy,T_cost_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(ew_returns_buy, ew_returns_sell, trade_days_summ_stats, non_trade_days_summ_stats, trade_days_T_cost, non_trade_days_T_cost):\n",
    "    print('Summary Statistics:')\n",
    "    print(' ')\n",
    "    SS = pd.DataFrame(columns=[' ','Buy', 'Sell'])\n",
    "    SS.loc[0, ' '] = 'Ann. Mean Alpha (without T-costs)'\n",
    "    SS.loc[0, 'Buy'] = trade_days_summ_stats[0]\n",
    "    SS.loc[0, 'Sell'] = non_trade_days_summ_stats[0]\n",
    "    SS.loc[1, ' '] = 'Ann. Tracking Error'\n",
    "    SS.loc[1, 'Buy'] = trade_days_summ_stats[1]\n",
    "    SS.loc[1, 'Sell'] = non_trade_days_summ_stats[1]\n",
    "    SS.loc[2, ' '] = 'Ann. Information Ratio (without T-costs)'\n",
    "    SS.loc[2, 'Buy'] = trade_days_summ_stats[2]\n",
    "    SS.loc[2, 'Sell'] = non_trade_days_summ_stats[2]\n",
    "    SS.loc[3, ' '] = 'Skewness of Alpha'\n",
    "    SS.loc[3, 'Buy'] = trade_days_summ_stats[3]\n",
    "    SS.loc[3, 'Sell'] = non_trade_days_summ_stats[3]\n",
    "    SS.loc[4, ' '] = '--------------------------'\n",
    "    SS.loc[4, 'Buy'] = '--'\n",
    "    SS.loc[4, 'Sell'] = '--'\n",
    "    SS.loc[5, ' '] = 'Ann. Mean Return (without T-costs)'\n",
    "    SS.loc[5, 'Buy'] = trade_days_summ_stats[4]\n",
    "    SS.loc[5, 'Sell'] = non_trade_days_summ_stats[4]\n",
    "    SS.loc[6, ' '] = 'Ann. Volatility'\n",
    "    SS.loc[6, 'Buy'] = trade_days_summ_stats[5]\n",
    "    SS.loc[6, 'Sell'] = non_trade_days_summ_stats[5]\n",
    "    SS.loc[7, ' '] = 'Ann. Sharpe Ratio (without T-costs)'\n",
    "    SS.loc[7, 'Buy'] = trade_days_summ_stats[6]\n",
    "    SS.loc[7, 'Sell'] = non_trade_days_summ_stats[6]\n",
    "    SS.loc[8, ' '] = 'Skewness of Return'\n",
    "    SS.loc[8, 'Buy'] = trade_days_summ_stats[7]\n",
    "    SS.loc[8, 'Sell'] = non_trade_days_summ_stats[7]\n",
    "    SS.loc[9, ' '] = 'Average Turnover'\n",
    "    SS.loc[9, 'Buy'] = np.mean(ew_returns_buy['turnover'])\n",
    "    SS.loc[9, 'Sell'] = np.mean(ew_returns_sell['turnover'])\n",
    "    SS.loc[10, ' '] = 'T-costs (on 1 unit of capital)'\n",
    "    SS.loc[10, 'Buy'] = trade_days_T_cost\n",
    "    SS.loc[10, 'Sell'] = non_trade_days_T_cost\n",
    "    SS = SS.set_index(' ').copy()\n",
    "    pd.set_option('display.expand_frame_repr', False, 'display.float_format', '{:,.2f}'.format)\n",
    "    print(SS)\n",
    "    print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plots_and_summary(ew_returns_buy,ew_returns_sell, summ_stats_buy, summ_stats_sell, T_cost_buy, T_cost_sell):    # New start: get performance plots\n",
    "    fig, axs = plt.subplots(2,2,figsize=(9,9))\n",
    "    \n",
    "    axs[0, 0].plot(ew_returns_buy['date'],ew_returns_buy['capital']) #cumulative_ret\n",
    "    axs[0, 0].set_xlabel('Date')\n",
    "    axs[0, 0].set_ylabel(\"Cumulative Return\")\n",
    "    axs[0, 0].set_title(\"Cum Return (net of T-cost) on buy days\")\n",
    "\n",
    "    axs[1, 0].plot(ew_returns_buy['date'],ew_returns_buy['alpha_capital']) #cumulative_ret\n",
    "    axs[1, 0].set_xlabel('Date')\n",
    "    axs[1, 0].set_ylabel(\"Cumulative Alpha\")\n",
    "    axs[1, 0].set_title(\"Cum Alpha (net of T-cost) on buy days\")\n",
    "    print('Number of buy periods', len(ew_returns_buy['capital']))\n",
    "    print('Check number of buy periods', len(ew_returns_buy['alpha_capital']))\n",
    "\n",
    "\n",
    "    axs[0, 1].plot(ew_returns_sell['date'],ew_returns_sell['capital'], color=\"r\") #cumulative_ret\n",
    "    axs[0, 1].set_xlabel('Date')\n",
    "    axs[0, 1].set_ylabel(\"Cumulative Return\")\n",
    "    axs[0, 1].set_title(\"Cum Return (net of T-cost) on sell days\")\n",
    "\n",
    "    axs[1, 1].plot(ew_returns_sell['date'],ew_returns_sell['alpha_capital'], color=\"r\") #cumulative_ret\n",
    "    axs[1, 1].set_xlabel('Date')\n",
    "    axs[1, 1].set_ylabel(\"Cumulative Alpha\")\n",
    "    axs[1, 1].set_title(\"Cum Alpha (net of T-cost) on sell days\")\n",
    "    print('Number of sell periods', len(ew_returns_sell['capital']))\n",
    "    print('Check number of sell periods', len(ew_returns_sell['alpha_capital']))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    print_summary(ew_returns_buy, ew_returns_sell, summ_stats_buy, summ_stats_sell, T_cost_buy, T_cost_sell)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FF_regression(regression_input):\n",
    "    y = regression_input['ew_return-RF']\n",
    "    X = regression_input[['Mkt-RF','SMB','HML','RMW','CMA','MOM']]\n",
    "    X = sm.add_constant(X) #intercept\n",
    "    model = sm.OLS(y,X)\n",
    "    results = model.fit()\n",
    "    print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one function to do it all, instead of repeating all these lines 5 times for each strategy we want to test\n",
    "def get_returns_turnover_Tcost_summary_plusPlots(data_smaller): #will input data_smaller, data_smaller5, data_smaller10, data_smaller15, and data_smaller20 in here\n",
    "    \n",
    "#FIRST GET FF6 factors    \n",
    "    FF_MOM = pd.read_csv('F-F_Momentum_Factor_daily.csv')[['date','MOM']]\n",
    "    for i in range(len(FF_MOM)):\n",
    "        FF_MOM.loc[i,'date'] = FF_MOM.loc[i,'date'].replace(',', '') #remove the comma from the date\n",
    "    FF_MOM['date'] = pd.to_datetime(FF_MOM['date'], format='%Y%m%d', errors='ignore')\n",
    "\n",
    "    FF5 = pd.read_csv('F-F_Research_Data_5_Factors_2x3_daily.csv').rename(columns={'Unnamed: 0':'date'})\n",
    "    FF5['date'] = pd.to_datetime(FF5['date'], format='%Y%m%d', errors='ignore')\n",
    "\n",
    "    FF6 = FF5.merge(FF_MOM, how='left', on='date').copy()\n",
    "    FF6[['Mkt-RF','SMB','HML','RMW','CMA','RF','MOM']] = FF6[['Mkt-RF','SMB','HML','RMW','CMA','RF','MOM']]/100\n",
    "    FF6\n",
    "    \n",
    "    \n",
    "#THEN GET PORTFOLIO RETURNS, TURNOVER, T_COST, AND SUMMARY STATS + PLOTS\n",
    "    ew_returns_buy, ew_returns_sell, data_buy, data_sell, summ_stats_buy, summ_stats_sell = get_portfolio_returns(data_smaller)\n",
    "\n",
    "    buy_regresion = ew_returns_buy[['date','ew_return']].merge(FF6, how='left', on='date').copy()\n",
    "    sell_regression = ew_returns_sell[['date','ew_return']].merge(FF6, how='left', on='date').copy()\n",
    "    buy_regresion['ew_return-RF'] = buy_regresion['ew_return'] - buy_regresion['RF']\n",
    "    sell_regression['ew_return-RF'] = sell_regression['ew_return'] - sell_regression['RF']\n",
    "    \n",
    "    print(\"\\033[1m\" + 'Regression for Buy Portfolio' + \"\\033[0m\")\n",
    "    FF_regression(buy_regresion)\n",
    "    print(\"\\033[1m\" + 'Regression for Sell Portfolio' + \"\\033[0m\")\n",
    "    FF_regression(sell_regression)\n",
    "    \n",
    "    turnover_buy2 = turnover(data_buy) #get turnover\n",
    "    turnover_sell2 = turnover(data_sell) #get turnover\n",
    "    ew_returns_buy['turnover'] = turnover_buy2['turnover']\n",
    "    ew_returns_sell['turnover'] = turnover_sell2['turnover']\n",
    "    ew_returns_sell\n",
    "\n",
    "    ew_returns_buy,T_cost_buy = T_cost_and_capital(ew_returns_buy)\n",
    "    ew_returns_buy\n",
    "    ew_returns_sell,T_cost_sell = T_cost_and_capital(ew_returns_sell)\n",
    "    ew_returns_sell\n",
    "\n",
    "    get_plots_and_summary(ew_returns_buy,ew_returns_sell, summ_stats_buy, summ_stats_sell, T_cost_buy, T_cost_sell)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
